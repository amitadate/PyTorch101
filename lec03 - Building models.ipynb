{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build a simple deep learning model using Torch. We have already encountered $autograd$ in the previous lecture. $nn$ is built on top of $autograd$ and contains the basic pre-built neural network layers such as Convolution, Pooling, Recurrent and Dropout layers. \n",
    "\n",
    "$nn.functional$ -> contains components functions of neural networks like convolution, pooling and activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Below is the model details for the architecture:\n",
    "        # 1 input image channel, 5 output channels, 3x3 square convolution kernel\n",
    "        # The first (Convolution) COV layer has 1 input channel, 5 output channels and a 3x3 kernel\n",
    "        self.conv1 = nn.Conv2d(1, 5, 3)\n",
    "        # The second COV layer has 5 input channel, 16 output channels and a 3x3 kernel\n",
    "        self.conv2 = nn.Conv2d(5, 16, 3)\n",
    "        # The next three layers are fully connected (FC) layers an affine operation: y = Wx + b\n",
    "        # The first layer has 16*3*3 input channels and 120 output channels\n",
    "        self.fc1 = nn.Linear(16 * 3 * 3, 120)\n",
    "        # The second layer has 120 input channels and 60 output channels\n",
    "        self.fc2 = nn.Linear(120, 60)\n",
    "        # The third layer has 60 input channels and 10 output channels\n",
    "        self.fc3 = nn.Linear(60, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # We can do the same thing by passing a single digit 2 \n",
    "        # Instead of a tuple (2,2) since the window is a square\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # We flatten the features produced from pooling and pass them to an FC layer\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        # We apply ReLU on the first FC layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # We apply ReLU on the second FC layer\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # We store and return the last FC layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        # For and n dimension tensor we return the product of all dimensions\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a class of the model with the required architecture is created.\n",
    "\n",
    "Remember: We only had to define the forward function since the backward function (where gradients are calculated) is automatically defined and taken care of my PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d (1, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d (5, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=144, out_features=120)\n",
      "  (fc2): Linear(in_features=120, out_features=60)\n",
      "  (fc3): Linear(in_features=60, out_features=10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is printed and we can easily see the architecture that we had created. Let us now use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $.parameters()$ function will return all the learnable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(params[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the first learnable parameter is a $5\\times1\\times3\\times3$ tensor since the first COV layer has 1 input channel, 5 output channels and a $3\\times3$ kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us give an example input of $20\\times20$ (a single color channel, Grayscale, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.0992 -0.0789  0.0040  0.0058 -0.0659 -0.0830  0.1925  0.0286 -0.0760 -0.0965\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.randn(1, 1, 20, 20))\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This $out$ vector is the output of the neural network.\n",
    "\n",
    "Till now, we learnt to make and use a simple neural network. Now we will delve into the loss function and get a deeper insight of how torch supports more complex neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we want to set our gradients to $0$ to initialize them. Torch supports a function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another practice that is not so commonly used is to backpropagate with random gradients. But if you ever need to do it, this is how simple it is.\n",
    "\n",
    "In the next lecture we will build a feedforward NN using torch and run it on our GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
