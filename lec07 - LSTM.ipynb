{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture we will build a Long short-term memory (LSTM) based NN and use the same dataset i.e. MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the training and test data from MNIST. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),  \n",
    "                            download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These will get saved in the $data$ folder (Should already be present from lecture 2) and their processed forms will be stored in the $data/processed$ folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is setting up the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input image size (no of columns)\n",
    "sequence_length = 28\n",
    "# Input image size (no of rows)\n",
    "input_size = 28\n",
    "# Size of the the hidden layer\n",
    "hidden_size = 128\n",
    "# How many recurrent layers do we want\n",
    "num_layers = 2\n",
    "# 10 classes since 10 digits\n",
    "num_classes = 10\n",
    "# Depending on how powerful your machine is you can increase the batch size\n",
    "batch_size = 100\n",
    "# Let us train it for 5 epochs i.e. 10 training cycles\n",
    "num_epochs = 5\n",
    "# To avoid overfitting, we start with a low learning rate\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have procured the data and set the hyperparameters, we will set up the pipeline for the input as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preparation phase is over, we have the data, a configured pipeline and now we will create the CNN model. \n",
    "\n",
    "We will create a 2 LSTM layer model with Batch normalization, ReLU and Max Pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # We create a 2 layer LSTM with input size of 28 (since 28 channels for each row)\n",
    "        # And a hidden layer size of 128\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # Output of each layer is fed as input to the next layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Output of each layer is fed as input to the next layer\n",
    "        # Set initial states to zero\n",
    "        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        c0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size))\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out = self.lstm(x, (h0, c0))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        # Decode hidden state of last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on creating your first LSTM model. Go ahead and experiment with the __learning rate__, __batch size__ and __number of epochs__ once you are done to understand how they affect your model's performance.\n",
    "\n",
    "Here we pass each $28\\times28$ image as $28$ rows with a length of $28$ each. Go ahead and try another approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(input_size, hidden_size, num_layers, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we used a fully connected layer and the outermost layers and used a ReLU activation in the hidden layer.\n",
    "\n",
    "Now that we have our model we need to decide what loss function we want to use and the optimizer that will minimize our loss function. We choose the Cross Entropy (or log) loss function and the [Adam Optimizer](https://arxiv.org/abs/1412.6980v8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the data i.e. we iterate over the test data and pass it to the model. We calculate loss, backpropagate and then try to optimize our loss function.\n",
    "\n",
    "A common practice while training is to print the status and the value of the loss after every $n$ steps. Here we choose $100$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get an estimate of how long this process takes using the $time$ package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Iteration [100/600], Loss: 0.5682\n",
      "Epoch [1/5], Iteration [200/600], Loss: 0.2132\n",
      "Epoch [1/5], Iteration [300/600], Loss: 0.3123\n",
      "Epoch [1/5], Iteration [400/600], Loss: 0.1894\n",
      "Epoch [1/5], Iteration [500/600], Loss: 0.2319\n",
      "Epoch [1/5], Iteration [600/600], Loss: 0.0868\n",
      "Epoch [2/5], Iteration [100/600], Loss: 0.1751\n",
      "Epoch [2/5], Iteration [200/600], Loss: 0.0617\n",
      "Epoch [2/5], Iteration [300/600], Loss: 0.1325\n",
      "Epoch [2/5], Iteration [400/600], Loss: 0.0404\n",
      "Epoch [2/5], Iteration [500/600], Loss: 0.0960\n",
      "Epoch [2/5], Iteration [600/600], Loss: 0.0485\n",
      "Epoch [3/5], Iteration [100/600], Loss: 0.0653\n",
      "Epoch [3/5], Iteration [200/600], Loss: 0.0883\n",
      "Epoch [3/5], Iteration [300/600], Loss: 0.0746\n",
      "Epoch [3/5], Iteration [400/600], Loss: 0.1601\n",
      "Epoch [3/5], Iteration [500/600], Loss: 0.1081\n",
      "Epoch [3/5], Iteration [600/600], Loss: 0.0699\n",
      "Epoch [4/5], Iteration [100/600], Loss: 0.0679\n",
      "Epoch [4/5], Iteration [200/600], Loss: 0.0089\n",
      "Epoch [4/5], Iteration [300/600], Loss: 0.0421\n",
      "Epoch [4/5], Iteration [400/600], Loss: 0.0265\n",
      "Epoch [4/5], Iteration [500/600], Loss: 0.0758\n",
      "Epoch [4/5], Iteration [600/600], Loss: 0.0281\n",
      "Epoch [5/5], Iteration [100/600], Loss: 0.0539\n",
      "Epoch [5/5], Iteration [200/600], Loss: 0.0724\n",
      "Epoch [5/5], Iteration [300/600], Loss: 0.1143\n",
      "Epoch [5/5], Iteration [400/600], Loss: 0.0314\n",
      "Epoch [5/5], Iteration [500/600], Loss: 0.0190\n",
      "Epoch [5/5], Iteration [600/600], Loss: 0.0911\n",
      "449.70111775398254\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Convert torch tensor to Variable for calculations\n",
    "        images = Variable(images.view(-1, sequence_length, input_size))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Forward\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs = lstm(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Iteration [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After $5$ epochs we get a loss of $0.09$ and the overall training time was $450$ seconds on the CPU. \n",
    "\n",
    "Note: Total number of iterations = size_of_dataset / batch_size \n",
    "\n",
    "Congratualtions on training your first model with an image dataset. No go ahead and test the model's performance and repeat this process after changing the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "lstm.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, sequence_length, input_size))\n",
    "    outputs = lstm(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are finally satisfied with your results you can even save your model for future use as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm.state_dict(), 'mnist_lstm.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
