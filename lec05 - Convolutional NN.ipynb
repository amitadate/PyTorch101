{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture we will build a Convolutional NN (CNN), we will use the same dataset i.e. MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the training and test data from MNIST. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),  \n",
    "                            download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These will get saved in the $data$ folder (Should already be present from lecture 2) and their processed forms will be stored in the $data/processed$ folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is setting up the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 10 classes since 10 digits\n",
    "num_classes = 10\n",
    "# Let us train it for 10 epochs i.e. 10 training cycles\n",
    "num_epochs = 10\n",
    "# Depending on how powerful your machine is you can increase the batch size\n",
    "batch_size = 100\n",
    "# To avoid overfitting, we start with a low learning rate\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have procured the data and set the hyperparameters, we will set up the pipeline for the input as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preparation phase is over, we have the data, a configured pipeline and now we will create the CNN model. \n",
    "\n",
    "We will create a 2 CNN layer model with Batch normalization, ReLU and Max Pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # For the 1st CNN layer we use 1 input image channel,\n",
    "        # 16 output channels, Kernel of 5x5 and Padding of 2\n",
    "        # Followed by BatchNorm, ReLU activation and Max Pooling\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        # For the 2nd CNN layer we use 16 input channel (output of layer 1),\n",
    "        # 32 output channels, Kernel of 5x5 and Padding of 2\n",
    "        # Followed by BatchNorm, ReLU activation and Max Pooling        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        # The last layer is a FC layer with 10 output channels (1 for each class label)\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Output of each layer is fed as input to the next layer\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on creating your first model. Go ahead and experiment with the __learning rate__, __kernel size__, __batch size__ and __number of epochs__ once you are done to understand how they affect your model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemant/anaconda3/lib/python3.6/site-packages/torch/cuda/__init__.py:87: UserWarning: \n",
      "    Found GPU0 GeForce GTX 1060 with Max-Q Design which requires CUDA_VERSION >= 8000 for\n",
      "     optimal performance and fast startup time, but your PyTorch was compiled\n",
      "     with CUDA_VERSION 7050. Please install the correct PyTorch binary\n",
      "     using instructions from http://pytorch.org\n",
      "    \n",
      "  warnings.warn(error_str % (d, name, 8000, CUDA_VERSION))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d (1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d (16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (fc): Linear(in_features=1568, out_features=10)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "cnn.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transfered the NN to the GPU using $cnn.cuda()$. Also note that we used a fully connected layer and the outermost layers and used a ReLU activation in the hidden layer.\n",
    "\n",
    "Now that we have our model we need to decide what loss function we want to use and the optimizer that will minimize our loss function. We choose the Cross Entropy (or log) loss function and the [Adam Optimizer](https://arxiv.org/abs/1412.6980v8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the data i.e. we iterate over the test data and pass it to the model. We calculate loss, backpropagate and then try to optimize our loss function.\n",
    "\n",
    "A common practice while training is to print the status and the value of the loss after every $n$ steps. Here we choose $100$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get an estimate of how long this process takes using the $time$ package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Iter [100/600], Loss: 0.2073\n",
      "Epoch [1/10], Iter [200/600], Loss: 0.1442\n",
      "Epoch [1/10], Iter [300/600], Loss: 0.0766\n",
      "Epoch [1/10], Iter [400/600], Loss: 0.1518\n",
      "Epoch [1/10], Iter [500/600], Loss: 0.0589\n",
      "Epoch [1/10], Iter [600/600], Loss: 0.0371\n",
      "Epoch [2/10], Iter [100/600], Loss: 0.0241\n",
      "Epoch [2/10], Iter [200/600], Loss: 0.0289\n",
      "Epoch [2/10], Iter [300/600], Loss: 0.0152\n",
      "Epoch [2/10], Iter [400/600], Loss: 0.0381\n",
      "Epoch [2/10], Iter [500/600], Loss: 0.0418\n",
      "Epoch [2/10], Iter [600/600], Loss: 0.0224\n",
      "Epoch [3/10], Iter [100/600], Loss: 0.0374\n",
      "Epoch [3/10], Iter [200/600], Loss: 0.0297\n",
      "Epoch [3/10], Iter [300/600], Loss: 0.0622\n",
      "Epoch [3/10], Iter [400/600], Loss: 0.0257\n",
      "Epoch [3/10], Iter [500/600], Loss: 0.0633\n",
      "Epoch [3/10], Iter [600/600], Loss: 0.0187\n",
      "Epoch [4/10], Iter [100/600], Loss: 0.0128\n",
      "Epoch [4/10], Iter [200/600], Loss: 0.0022\n",
      "Epoch [4/10], Iter [300/600], Loss: 0.0024\n",
      "Epoch [4/10], Iter [400/600], Loss: 0.0410\n",
      "Epoch [4/10], Iter [500/600], Loss: 0.0042\n",
      "Epoch [4/10], Iter [600/600], Loss: 0.0146\n",
      "Epoch [5/10], Iter [100/600], Loss: 0.0994\n",
      "Epoch [5/10], Iter [200/600], Loss: 0.0192\n",
      "Epoch [5/10], Iter [300/600], Loss: 0.0043\n",
      "Epoch [5/10], Iter [400/600], Loss: 0.0216\n",
      "Epoch [5/10], Iter [500/600], Loss: 0.0170\n",
      "Epoch [5/10], Iter [600/600], Loss: 0.0069\n",
      "Epoch [6/10], Iter [100/600], Loss: 0.0384\n",
      "Epoch [6/10], Iter [200/600], Loss: 0.0277\n",
      "Epoch [6/10], Iter [300/600], Loss: 0.0025\n",
      "Epoch [6/10], Iter [400/600], Loss: 0.0156\n",
      "Epoch [6/10], Iter [500/600], Loss: 0.0590\n",
      "Epoch [6/10], Iter [600/600], Loss: 0.0133\n",
      "Epoch [7/10], Iter [100/600], Loss: 0.0019\n",
      "Epoch [7/10], Iter [200/600], Loss: 0.0703\n",
      "Epoch [7/10], Iter [300/600], Loss: 0.0908\n",
      "Epoch [7/10], Iter [400/600], Loss: 0.0031\n",
      "Epoch [7/10], Iter [500/600], Loss: 0.0027\n",
      "Epoch [7/10], Iter [600/600], Loss: 0.0793\n",
      "Epoch [8/10], Iter [100/600], Loss: 0.0127\n",
      "Epoch [8/10], Iter [200/600], Loss: 0.0177\n",
      "Epoch [8/10], Iter [300/600], Loss: 0.0268\n",
      "Epoch [8/10], Iter [400/600], Loss: 0.0026\n",
      "Epoch [8/10], Iter [500/600], Loss: 0.0083\n",
      "Epoch [8/10], Iter [600/600], Loss: 0.0241\n",
      "Epoch [9/10], Iter [100/600], Loss: 0.0019\n",
      "Epoch [9/10], Iter [200/600], Loss: 0.0027\n",
      "Epoch [9/10], Iter [300/600], Loss: 0.0063\n",
      "Epoch [9/10], Iter [400/600], Loss: 0.0169\n",
      "Epoch [9/10], Iter [500/600], Loss: 0.0097\n",
      "Epoch [9/10], Iter [600/600], Loss: 0.0413\n",
      "Epoch [10/10], Iter [100/600], Loss: 0.0101\n",
      "Epoch [10/10], Iter [200/600], Loss: 0.0027\n",
      "Epoch [10/10], Iter [300/600], Loss: 0.0439\n",
      "Epoch [10/10], Iter [400/600], Loss: 0.0144\n",
      "Epoch [10/10], Iter [500/600], Loss: 0.0074\n",
      "Epoch [10/10], Iter [600/600], Loss: 0.0018\n",
      "172.65085411071777\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Convert torch tensor to Variable for calculations\n",
    "        images = Variable(images.cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "        \n",
    "        # Forward\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 10 epochs we get a loss of $0.002$ and the overall training time was $173$ seconds. \n",
    "\n",
    "Note: Total number of iterations = size_of_dataset / batch_size \n",
    "\n",
    "Congratualtions on training your first model with an image dataset. No go ahead and test the model's performance and repeat this process after changing the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 99 %\n"
     ]
    }
   ],
   "source": [
    "cnn.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.cuda())\n",
    "    outputs = cnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels).sum()\n",
    "    \n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are finally satisfied with your results you can even save your model for future use as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cnn.state_dict(), 'mnist_cnn.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
